2025-03-03 01:01:22,957 - __main__ - INFO - Starting test generation process
2025-03-03 01:02:52,294 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 403 Forbidden"
2025-03-03 01:07:35,729 - __main__ - INFO - Starting test generation process
2025-03-03 01:07:38,303 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 403 Forbidden"
2025-03-03 01:08:11,538 - __main__ - INFO - Starting test generation process
2025-03-03 01:08:13,133 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 403 Forbidden"
2025-03-03 01:08:57,934 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 403 Forbidden"
2025-03-03 01:11:21,366 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 01:11:25,502 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 01:11:28,566 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 01:11:30,256 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 01:11:33,075 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 01:11:38,645 - root - ERROR - Error loading prompt from prompts/base_case.txt: [Errno 21] Is a directory: 'Formatted_data/Text_files'
2025-03-03 09:41:58,377 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 09:42:00,849 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 09:42:01,867 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 09:42:04,407 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 09:42:07,118 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 10:41:00,393 - __main__ - INFO - Starting test generation process
2025-03-03 10:41:05,454 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 10:41:07,814 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 10:41:10,453 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 10:41:13,502 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 10:41:15,804 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 10:42:03,761 - __main__ - INFO - Starting test generation process
2025-03-03 10:42:08,486 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 10:42:11,325 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 10:42:13,937 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 10:42:16,583 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 10:42:18,953 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 10:42:56,110 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 10:43:19,719 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-03-03 10:43:19,735 - __main__ - INFO - Response generated with 8 source documents
2025-03-03 10:43:19,735 - __main__ - INFO - Source documents: ['fate.core.PlatformConnectors.jira.testcase.txt', 'fate.core.PlatformConnectors.jira.testcaseresult.txt', 'TestScriptSteps.txt', 'StepItem.txt', 'fate.core.PlatformConnectors.jira.testcycle.txt', 'Uses of Package fate.core.PlatformConnectors.jira.testcase.txt', 'IResultPlatformHandler.txt', 'DirectoryControl.txt']
2025-03-03 11:43:57,512 - __main__ - INFO - Starting prompt-based code generation
2025-03-03 11:43:57,514 - __main__ - ERROR - Error in prompt inference pipeline: 1 validation error for ChatOpenAI
  Value error, Parameters {'timeout'} should be specified explicitly. Instead they were passed in as part of `model_kwargs` parameter. [type=value_error, input_value={'model': 'gpt-4o', 'temp...enai', 'max_retries': 3}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/value_error
2025-03-03 11:43:57,514 - root - ERROR - Error running prompt inference: 1 validation error for ChatOpenAI
  Value error, Parameters {'timeout'} should be specified explicitly. Instead they were passed in as part of `model_kwargs` parameter. [type=value_error, input_value={'model': 'gpt-4o', 'temp...enai', 'max_retries': 3}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/value_error
2025-03-03 11:44:56,345 - __main__ - INFO - Starting prompt-based code generation
2025-03-03 11:44:56,345 - __main__ - ERROR - Error in prompt inference pipeline: 1 validation error for ChatOpenAI
  Value error, Parameters {'timeout'} should be specified explicitly. Instead they were passed in as part of `model_kwargs` parameter. [type=value_error, input_value={'model': 'openai-main/gp...enai', 'max_retries': 3}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/value_error
2025-03-03 11:44:56,345 - root - ERROR - Error running prompt inference: 1 validation error for ChatOpenAI
  Value error, Parameters {'timeout'} should be specified explicitly. Instead they were passed in as part of `model_kwargs` parameter. [type=value_error, input_value={'model': 'openai-main/gp...enai', 'max_retries': 3}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/value_error
2025-03-03 11:45:03,722 - __main__ - INFO - Starting prompt-based code generation
2025-03-03 11:45:03,723 - __main__ - ERROR - Error in prompt inference pipeline: 1 validation error for ChatOpenAI
  Value error, Parameters {'timeout'} should be specified explicitly. Instead they were passed in as part of `model_kwargs` parameter. [type=value_error, input_value={'model': 'openai-main/gp...enai', 'max_retries': 3}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/value_error
2025-03-03 11:45:38,536 - __main__ - INFO - Starting prompt-based code generation
2025-03-03 11:45:38,868 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 401 Unauthorized"
2025-03-03 11:45:38,870 - __main__ - ERROR - Error generating code: Error code: 401 - {'status': 'failure', 'message': 'Forbidden: Invalid token'}
2025-03-03 11:45:38,870 - __main__ - ERROR - Error in prompt inference pipeline: Error code: 401 - {'status': 'failure', 'message': 'Forbidden: Invalid token'}
2025-03-03 11:46:02,012 - __main__ - INFO - Starting prompt-based code generation
2025-03-03 11:46:02,658 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 11:46:35,449 - __main__ - INFO - Successfully generated and saved code to data/prompt_inference/generated_code.java
2025-03-03 11:46:35,450 - __main__ - ERROR - Error in prompt inference pipeline: [Errno 21] Is a directory: 'Formatted_data/Ground_Truths'
2025-03-03 11:48:25,752 - __main__ - INFO - Starting test generation process
2025-03-03 11:48:31,513 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 11:48:33,662 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 11:48:36,221 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 11:48:38,884 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 11:48:41,599 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 12:23:15,511 - __main__ - INFO - Starting prompt-based code generation
2025-03-03 12:23:15,511 - root - ERROR - Error loading prompt from prompts/base_case.txt: [Errno 2] No such file or directory: 'Formatted_data/Text_files/838.txt'
2025-03-03 12:23:15,511 - __main__ - ERROR - Error in prompt inference pipeline: [Errno 2] No such file or directory: 'Formatted_data/Text_files/838.txt'
2025-03-03 12:23:15,511 - root - ERROR - Error running prompt inference: [Errno 2] No such file or directory: 'Formatted_data/Text_files/838.txt'
2025-03-03 12:25:20,761 - __main__ - INFO - Starting prompt-based code generation
2025-03-03 12:25:20,764 - __main__ - ERROR - Error in prompt inference pipeline: 1 validation error for ChatOpenAI
  Value error, Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. [type=value_error, input_value={'temperature': 0.3, 'max...ne, 'http_client': None}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/value_error
2025-03-03 12:25:20,765 - root - ERROR - Error running prompt inference: 1 validation error for ChatOpenAI
  Value error, Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. [type=value_error, input_value={'temperature': 0.3, 'max...ne, 'http_client': None}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/value_error
2025-03-03 12:27:03,620 - __main__ - INFO - Starting prompt-based code generation
2025-03-03 12:27:05,209 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 12:27:33,929 - __main__ - INFO - Successfully generated and saved code to data/prompt_inference/generated_code.java
2025-03-03 12:27:33,930 - __main__ - ERROR - Error in prompt inference pipeline: [Errno 21] Is a directory: 'Formatted_data/Ground_Truths'
2025-03-03 12:27:33,930 - root - ERROR - Error running prompt inference: [Errno 21] Is a directory: 'Formatted_data/Ground_Truths'
2025-03-03 12:33:09,799 - __main__ - INFO - Starting prompt-based code generation
2025-03-03 12:33:11,121 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 12:33:56,005 - __main__ - INFO - Successfully generated and saved code to data/prompt_inference/generated_code.java
2025-03-03 12:34:08,674 - absl - INFO - Using default tokenizer.
2025-03-03 12:34:40,116 - __main__ - INFO - Code generation completed successfully
2025-03-03 12:34:40,116 - __main__ - INFO - Generation Results: {'base_metrics': {'rouge': {'rouge1': 0.3323442136498516, 'rouge2': 0.15453194650817234, 'rougeL': 0.15281899109792282, 'rougeLsum': 0.32789317507418403}, 'meteor': {'meteor': 0.36625427649486547}, 'chrf': {'score': 38.67162997274549, 'char_order': 6, 'word_order': 0, 'beta': 2}, 'levenshtein_similarity': 0.2462842402653237}, 'codebleu_metrics': {'codebleu': 0.3430730333274065, 'ngram_match_score': 0.10619874800172716, 'weighted_ngram_match_score': 0.13757052400213327, 'syntax_match_score': 0.6309859154929578, 'dataflow_match_score': 0.4975369458128079}, 'codebert_metrics': {'precision': 0.7205706834793091, 'recall': 0.7798356413841248, 'f1': 0.749032735824585, 'f3': 0.7734740376472473}}
2025-03-03 13:11:08,449 - __main__ - INFO - Starting prompt-based code generation
2025-03-03 13:11:09,818 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 13:11:34,286 - __main__ - INFO - Successfully generated and saved code to data/prompt_inference/generated_code.java
2025-03-03 13:11:42,257 - absl - INFO - Using default tokenizer.
2025-03-03 13:11:48,078 - __main__ - INFO - Code generation completed successfully
2025-03-03 13:11:48,078 - __main__ - INFO - Generation Results: {'base_metrics': {'rouge': {'rouge1': 0.23956043956043957, 'rouge2': 0.08370044052863436, 'rougeL': 0.17802197802197803, 'rougeLsum': 0.23736263736263735}, 'meteor': {'meteor': 0.3236416631242184}, 'chrf': {'score': 37.024981420931695, 'char_order': 6, 'word_order': 0, 'beta': 2}, 'levenshtein_similarity': 0.2899739843906344}, 'codebleu_metrics': {'codebleu': 0.22573147305461388, 'ngram_match_score': 0.026898799913214172, 'weighted_ngram_match_score': 0.03814350898337541, 'syntax_match_score': 0.40532544378698226, 'dataflow_match_score': 0.4325581395348837}, 'codebert_metrics': {'precision': 0.7155088186264038, 'recall': 0.7500744462013245, 'f1': 0.7323839664459229, 'f3': 0.7464683055877686}}
2025-03-03 13:31:35,699 - prompt_inference - INFO - Starting prompt-based code generation
2025-03-03 13:31:36,950 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 13:32:06,663 - prompt_inference - INFO - Successfully generated and saved code to data/prompt_inference/generated_code.java
2025-03-03 13:32:15,613 - absl - INFO - Using default tokenizer.
2025-03-03 13:32:20,831 - prompt_inference - INFO - Code generation completed successfully
2025-03-03 14:11:04,139 - __main__ - INFO - Starting test generation process
2025-03-03 14:11:09,680 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:11:12,484 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:11:14,968 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:11:16,686 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:11:18,447 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:11:19,046 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:15:54,486 - __main__ - INFO - Starting test generation process
2025-03-03 14:15:59,335 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:16:02,372 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:16:04,921 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:16:07,080 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:16:09,683 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:16:10,611 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:17:27,056 - __main__ - INFO - Starting test generation process
2025-03-03 14:17:31,210 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:17:34,068 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:17:36,169 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:17:39,261 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:17:41,807 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:17:42,912 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:18:03,017 - __main__ - INFO - Starting test generation process
2025-03-03 14:18:07,432 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:18:10,073 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:18:12,288 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:18:14,860 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:18:17,387 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:18:18,520 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:19:15,077 - __main__ - INFO - Starting test generation process
2025-03-03 14:19:19,409 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:19:22,068 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:19:24,055 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:19:26,460 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:19:28,723 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:19:29,553 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:20:54,978 - __main__ - INFO - Starting test generation process
2025-03-03 14:20:59,282 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:21:02,271 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:21:04,466 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:21:06,616 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:21:08,575 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:21:09,448 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:26:25,844 - __main__ - INFO - Starting test generation process
2025-03-03 14:26:30,155 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:26:32,748 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:26:34,235 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:26:36,550 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:26:39,134 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:26:39,945 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:28:10,511 - __main__ - INFO - Starting test generation process
2025-03-03 14:28:14,074 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:28:16,585 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:28:19,281 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:28:21,793 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:28:24,117 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:28:26,917 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:30:48,746 - __main__ - INFO - Starting test generation process
2025-03-03 14:30:53,146 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:30:57,036 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:31:00,418 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:31:02,521 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:31:05,137 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:31:05,818 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:31:10,814 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 14:31:10,822 - __main__ - INFO - Response generated with 8 source documents
2025-03-03 14:31:10,822 - __main__ - INFO - Source documents: ['fate.core.PlatformConnectors.jira.testcaseresult.txt', 'fate.core.PlatformConnectors.jira.testcase.txt', 'TestScriptSteps.txt', 'StepItem.txt', 'CoreBrowserControl.txt', 'DirectoryControl.txt', 'CoreUI.txt', 'fate.core.PlatformConnectors.jira.testcycle.txt']
2025-03-03 14:32:57,250 - __main__ - INFO - Starting test generation process
2025-03-03 14:33:01,878 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:33:04,606 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:33:07,007 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:33:09,684 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:33:11,880 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:33:12,759 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:33:17,520 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 14:33:17,524 - __main__ - INFO - Response generated with 8 source documents
2025-03-03 14:33:17,524 - __main__ - INFO - Source documents: ['fate.core.PlatformConnectors.jira.testcaseresult.txt', 'fate.core.PlatformConnectors.jira.testcase.txt', 'TestScriptSteps.txt', 'StepItem.txt', 'CoreBrowserControl.txt', 'DirectoryControl.txt', 'CoreUI.txt', 'CoreMouseControl.txt']
2025-03-03 14:35:02,035 - __main__ - INFO - Starting test generation process
2025-03-03 14:35:06,339 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:35:08,817 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:35:12,287 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:35:14,083 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:35:16,282 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:35:17,181 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:35:20,946 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 14:35:20,955 - __main__ - INFO - Response generated with 8 source documents
2025-03-03 14:35:20,956 - __main__ - INFO - Source documents: ['fate.core.PlatformConnectors.jira.testcaseresult.txt', 'fate.core.PlatformConnectors.jira.testcase.txt', 'TestScriptSteps.txt', 'StepItem.txt', 'CoreBrowserControl.txt', 'DirectoryControl.txt', 'CoreUI.txt', 'CoreMouseControl.txt']
2025-03-03 14:35:29,165 - absl - INFO - Using default tokenizer.
2025-03-03 14:35:33,273 - __main__ - INFO - Test generation completed
2025-03-03 14:41:29,190 - __main__ - INFO - Starting test generation process
2025-03-03 14:41:33,206 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:41:36,515 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:41:38,688 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:41:41,642 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:41:43,107 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:41:43,854 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:42:19,714 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 14:42:46,281 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 14:43:10,954 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 14:43:44,372 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 14:44:09,420 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 14:44:31,865 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 14:44:53,014 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 14:45:19,504 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 14:45:19,510 - __main__ - INFO - Response generated with 8 source documents
2025-03-03 14:45:19,510 - __main__ - INFO - Source documents: ['fate.core.PlatformConnectors.jira.testcase.txt', 'fate.core.PlatformConnectors.jira.testcaseresult.txt', 'TestScriptSteps.txt', 'StepItem.txt', 'CoreUI.txt', 'IResultPlatformHandler.txt', 'fate.core.PlatformConnectors.jira.testcycle.txt', 'DirectoryControl.txt']
2025-03-03 14:45:28,527 - __main__ - INFO - Starting test generation process
2025-03-03 14:45:33,196 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:45:36,431 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:45:38,862 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:45:40,575 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:45:43,047 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:45:43,688 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 14:46:04,086 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 14:46:23,672 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 14:46:45,998 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 14:47:07,458 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 14:47:32,849 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 14:47:51,316 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 14:48:11,784 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 14:48:30,720 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 14:48:30,723 - __main__ - INFO - Response generated with 8 source documents
2025-03-03 14:48:30,723 - __main__ - INFO - Source documents: ['fate.core.PlatformConnectors.jira.testcase.txt', 'fate.core.PlatformConnectors.jira.testcaseresult.txt', 'TestScriptSteps.txt', 'StepItem.txt', 'fate.core.PlatformConnectors.jira.testcycle.txt', 'IResultPlatformHandler.txt', 'DirectoryControl.txt', 'CoreUI.txt']
2025-03-03 14:48:53,427 - absl - INFO - Using default tokenizer.
2025-03-03 14:49:00,081 - __main__ - INFO - Test generation completed
<<<<<<< HEAD
2025-03-03 15:01:15,764 - __main__ - INFO - Starting test generation process
2025-03-03 15:01:20,912 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 15:01:23,630 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 15:01:25,799 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 15:01:28,406 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 15:01:30,851 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 15:01:31,912 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 15:02:02,468 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:02:20,195 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:02:40,082 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:03:10,044 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:03:39,180 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:03:57,745 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:04:17,293 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:04:43,858 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:04:43,863 - __main__ - INFO - Response generated with 8 source documents
2025-03-03 15:04:43,863 - __main__ - INFO - Source documents: ['fate.core.PlatformConnectors.jira.testcase.txt', 'fate.core.PlatformConnectors.jira.testcaseresult.txt', 'TestScriptSteps.txt', 'StepItem.txt', 'fate.core.PlatformConnectors.jira.testcycle.txt', 'IResultPlatformHandler.txt', 'DirectoryControl.txt', 'CoreUI.txt']
2025-03-03 15:04:51,979 - absl - INFO - Using default tokenizer.
2025-03-03 15:05:04,777 - __main__ - INFO - Test generation completed
2025-03-03 15:16:41,215 - prompt_inference - INFO - Starting prompt-based code generation
2025-03-03 15:16:43,424 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:17:04,299 - prompt_inference - INFO - Successfully generated and saved code to data/prompt_inference/generated_code.java
2025-03-03 15:17:04,300 - prompt_inference - ERROR - Error in prompt inference pipeline: [Errno 21] Is a directory: 'Formatted_data/Ground_Truths'
2025-03-03 15:17:48,553 - __main__ - INFO - Starting prompt-based code generation
2025-03-03 15:17:49,570 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:18:14,810 - __main__ - INFO - Successfully generated and saved code to data/prompt_inference/generated_code.java
2025-03-03 15:18:14,811 - __main__ - ERROR - Error in prompt inference pipeline: [Errno 21] Is a directory: 'Formatted_data/Ground_Truths'
2025-03-03 15:18:14,812 - root - ERROR - Error running prompt inference: [Errno 21] Is a directory: 'Formatted_data/Ground_Truths'
2025-03-03 15:21:17,045 - __main__ - INFO - Starting prompt-based code generation
2025-03-03 15:21:18,415 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:21:39,125 - __main__ - INFO - Successfully generated and saved code to data/prompt_inference/generated_code.java
2025-03-03 15:21:47,861 - absl - INFO - Using default tokenizer.
2025-03-03 15:21:53,984 - __main__ - INFO - Code generation completed successfully
2025-03-03 15:21:53,984 - __main__ - INFO - Generation Results: {'base_metrics': {'rouge': {'rouge1': 0.29770387965162315, 'rouge2': 0.126883425852498, 'rougeL': 0.20744259699129056, 'rougeLsum': 0.29612034837688045}, 'meteor': {'meteor': 0.413445510864184}, 'chrf': {'score': 41.63311929955937, 'char_order': 6, 'word_order': 0, 'beta': 2}, 'levenshtein_similarity': 0.26176145436678544}, 'codebleu_metrics': {'codebleu': 0.34245177893224776, 'ngram_match_score': 0.0825420510285854, 'weighted_ngram_match_score': 0.16086531013682234, 'syntax_match_score': 0.6792452830188679, 'dataflow_match_score': 0.44715447154471544}, 'codebert_metrics': {'precision': 0.7203366160392761, 'recall': 0.7727111577987671, 'f1': 0.7456052899360657, 'f3': 0.7671334743499756}}
2025-03-03 15:22:44,479 - prompt_inference - INFO - Starting prompt-based code generation
2025-03-03 15:22:45,164 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:23:03,927 - prompt_inference - INFO - Successfully generated and saved code to data/prompt_inference/generated_code.java
2025-03-03 15:23:11,714 - absl - INFO - Using default tokenizer.
2025-03-03 15:23:17,876 - prompt_inference - INFO - Code generation completed successfully
2025-03-03 15:24:08,891 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 15:24:11,264 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 15:24:13,719 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 15:24:16,348 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 15:24:18,559 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 15:24:22,419 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 15:24:43,827 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:25:04,049 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:25:30,186 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:25:49,432 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:26:04,635 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:26:29,408 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:26:55,342 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:27:11,161 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:27:11,165 - rag_llamaindex - INFO - Response generated with 8 source documents
2025-03-03 15:27:11,166 - rag_llamaindex - INFO - Source documents: ['fate.core.PlatformConnectors.jira.testcase.txt', 'fate.core.PlatformConnectors.jira.testcaseresult.txt', 'TestScriptSteps.txt', 'StepItem.txt', 'fate.core.PlatformConnectors.jira.testcycle.txt', 'IResultPlatformHandler.txt', 'CoreUI.txt', 'DirectoryControl.txt']
2025-03-03 15:31:54,902 - prompt_inference - INFO - Starting prompt-based code generation
2025-03-03 15:31:55,838 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:32:41,210 - prompt_inference - INFO - Successfully generated and saved code to data/prompt_inference/generated_code.java
2025-03-03 15:32:49,419 - absl - INFO - Using default tokenizer.
2025-03-03 15:32:56,486 - prompt_inference - INFO - Code generation completed successfully
2025-03-03 15:36:33,960 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 15:36:36,818 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 15:36:39,551 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 15:36:41,630 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 15:36:43,010 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 15:40:33,232 - prompt_inference - INFO - Starting prompt-based code generation
2025-03-03 15:40:34,013 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:41:01,015 - prompt_inference - INFO - Successfully generated and saved code to data/prompt_inference/generated_code.java
2025-03-03 15:41:17,697 - absl - INFO - Using default tokenizer.
2025-03-03 15:41:22,622 - prompt_inference - INFO - Code generation completed successfully
2025-03-03 15:41:22,624 - prompt_inference - INFO - Starting prompt-based code generation
2025-03-03 15:41:23,605 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:41:48,499 - prompt_inference - INFO - Successfully generated and saved code to data/prompt_inference/generated_code.java
2025-03-03 15:41:55,691 - absl - INFO - Using default tokenizer.
2025-03-03 15:42:00,050 - prompt_inference - INFO - Code generation completed successfully
2025-03-03 15:42:00,050 - prompt_inference - INFO - Starting prompt-based code generation
2025-03-03 15:42:00,709 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:42:28,576 - prompt_inference - INFO - Successfully generated and saved code to data/prompt_inference/generated_code.java
2025-03-03 15:42:36,878 - absl - INFO - Using default tokenizer.
2025-03-03 15:42:41,115 - prompt_inference - INFO - Code generation completed successfully
2025-03-03 15:42:41,117 - prompt_inference - INFO - Starting prompt-based code generation
2025-03-03 15:42:41,839 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:43:06,754 - prompt_inference - INFO - Successfully generated and saved code to data/prompt_inference/generated_code.java
2025-03-03 15:43:27,873 - absl - INFO - Using default tokenizer.
2025-03-03 15:43:31,815 - prompt_inference - INFO - Code generation completed successfully
2025-03-03 15:48:12,406 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 15:48:15,345 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 15:48:18,104 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 15:48:20,714 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 15:48:49,115 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 15:50:02,770 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 15:50:36,395 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:51:05,906 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:51:37,632 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:51:59,222 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:52:29,140 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:53:03,170 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:53:30,790 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:53:49,176 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:54:12,188 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:54:12,192 - rag_llamaindex - INFO - Response generated with 8 source documents
2025-03-03 15:54:12,193 - rag_llamaindex - INFO - Source documents: ['fate.core.PlatformConnectors.jira.testcase.txt', 'fate.core.PlatformConnectors.jira.testcaseresult.txt', 'TestScriptSteps.txt', 'StepItem.txt', 'fate.core.PlatformConnectors.jira.testcycle.txt', 'IResultPlatformHandler.txt', 'CoreUI.txt', 'DirectoryControl.txt']
2025-03-03 15:54:13,907 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 15:54:49,517 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:55:24,567 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:55:58,915 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:56:17,898 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:56:38,069 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:56:56,383 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:57:13,981 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:57:35,575 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:57:56,864 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:57:56,868 - rag_llamaindex - INFO - Response generated with 8 source documents
2025-03-03 15:57:56,868 - rag_llamaindex - INFO - Source documents: ['fate.core.PlatformConnectors.jira.testcase.txt', 'fate.core.PlatformConnectors.jira.testcaseresult.txt', 'TestScriptSteps.txt', 'StepItem.txt', 'fate.core.PlatformConnectors.jira.testcycle.txt', 'IResultPlatformHandler.txt', 'DirectoryControl.txt', 'CoreUI.txt']
2025-03-03 15:57:57,907 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-03 15:58:36,563 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:59:02,338 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 15:59:26,532 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 16:00:00,379 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 16:00:23,295 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 16:00:46,302 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-03 16:01:07,805 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
=======
2025-03-04 15:21:08,189 - __main__ - INFO - Starting test generation process
2025-03-04 15:21:17,415 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-04 15:21:21,305 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-04 15:21:24,545 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-04 15:21:28,676 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-04 15:21:31,670 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-04 15:21:32,880 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-04 15:21:57,315 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-04 15:22:24,037 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-04 15:22:45,264 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-04 15:23:13,188 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-04 15:23:39,593 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-04 15:24:12,312 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-04 15:24:36,991 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-04 15:24:36,994 - __main__ - INFO - Response generated with 8 source documents
2025-03-04 15:24:36,994 - __main__ - INFO - Source documents: ['fate.core.PlatformConnectors.jira.testcase.txt', 'fate.core.PlatformConnectors.jira.testcaseresult.txt', 'TestScriptSteps.txt', 'StepItem.txt', 'fate.core.PlatformConnectors.jira.testcycle.txt', 'Uses of Package fate.core.PlatformConnectors.jira.testcase.txt', 'IResultPlatformHandler.txt', 'DirectoryControl.txt']
2025-03-04 15:25:41,941 - absl - INFO - Using default tokenizer.
<<<<<<< HEAD
>>>>>>> 1980f98 (added print)
=======
2025-03-04 15:28:22,285 - __main__ - INFO - Starting prompt-based code generation
2025-03-04 15:28:23,350 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-04 15:28:53,847 - __main__ - INFO - Successfully generated and saved code to data/prompt_inference/generated_code.java
2025-03-04 15:28:53,847 - __main__ - ERROR - Error in prompt inference pipeline: [Errno 21] Is a directory: 'Formatted_data/Ground_Truths'
2025-03-04 15:28:53,847 - root - ERROR - Error running prompt inference: [Errno 21] Is a directory: 'Formatted_data/Ground_Truths'
2025-03-04 15:29:13,338 - absl - INFO - Using default tokenizer.
>>>>>>> b152310 (--)
2025-03-04 15:38:30,336 - prompt_inference - INFO - Starting prompt-based code generation
2025-03-04 15:38:31,471 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-04 15:38:56,280 - prompt_inference - INFO - Successfully generated and saved code to data/prompt_inference/generated_code.java
2025-03-04 15:39:09,024 - absl - INFO - Using default tokenizer.
2025-03-04 15:39:14,877 - prompt_inference - INFO - Code generation completed successfully
2025-03-04 15:39:42,709 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-04 15:39:47,411 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-04 15:39:50,278 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-04 15:39:54,180 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-04 15:39:57,251 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-04 15:40:00,254 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/embeddings "HTTP/1.1 200 OK"
2025-03-04 15:40:29,191 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-04 15:41:00,364 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-04 15:41:32,578 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-04 15:42:01,309 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-04 15:42:39,813 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-04 15:43:14,116 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-04 15:43:46,769 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-04 15:44:21,786 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
2025-03-04 15:44:22,111 - rag_llamaindex - INFO - Response generated with 8 source documents
2025-03-04 15:44:22,111 - rag_llamaindex - INFO - Source documents: ['fate.core.PlatformConnectors.jira.testcase.txt', 'fate.core.PlatformConnectors.jira.testcaseresult.txt', 'TestScriptSteps.txt', 'StepItem.txt', 'fate.core.PlatformConnectors.jira.testcycle.txt', 'Uses of Package fate.core.PlatformConnectors.jira.testcase.txt', 'IResultPlatformHandler.txt', 'DirectoryControl.txt']
2025-03-04 19:45:17,822 - __main__ - INFO - Starting prompt-based code generation
2025-03-04 19:45:19,943 - httpx - INFO - HTTP Request: POST https://llm-gateway.truefoundry.com/api/inference/openai/chat/completions "HTTP/1.1 200 OK"
